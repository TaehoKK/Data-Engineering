{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "771ecc53-f3a5-426f-94ce-9e528d668bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafka - spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c326f16a-2743-4d8f-8a07-4f5d69682130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, streaming\n",
    "from pyspark.sql.functions import col,expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e5fa62-c724-496a-84ec-9ae1e737a49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/03 16:29:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# create session\n",
    "spark = SparkSession.builder.appName('kafka_spark').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cd9de53-1f0b-4f1d-8f0f-eaa09755b957",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/03 15:45:20 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-45e6adf4-d3a8-4dcd-86c7-f86b0447cd13. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/03 15:45:20 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7fcd28145580>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/03 15:45:20 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 48\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+----+-----------+\n",
      "| topic| key|      value|\n",
      "+------+----+-----------+\n",
      "|topic2|NULL|\"Alejandro\"|\n",
      "+------+----+-----------+\n",
      "\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 320\n",
      "-------------------------------------------\n",
      "+------+----+------------------+\n",
      "| topic| key|             value|\n",
      "+------+----+------------------+\n",
      "|topic1|NULL|0.6539328077202688|\n",
      "+------+----+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 104\n",
      "-------------------------------------------\n",
      "+------+----+------------------+\n",
      "| topic| key|             value|\n",
      "+------+----+------------------+\n",
      "|topic1|NULL|0.6539328077202688|\n",
      "+------+----+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 321\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 49\n",
      "-------------------------------------------\n",
      "+------+----+------------------+\n",
      "| topic| key|             value|\n",
      "+------+----+------------------+\n",
      "|topic1|NULL|1.5078878639480642|\n",
      "+------+----+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n",
      "+------+----+------------------+\n",
      "| topic| key|             value|\n",
      "+------+----+------------------+\n",
      "|topic1|NULL|1.5078878639480642|\n",
      "+------+----+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 105\n",
      "-------------------------------------------\n",
      "+------+----+------------------+\n",
      "| topic| key|             value|\n",
      "+------+----+------------------+\n",
      "|topic1|NULL|1.5078878639480642|\n",
      "+------+----+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 322\n",
      "-------------------------------------------\n",
      "+------+----+--------------------+\n",
      "| topic| key|               value|\n",
      "+------+----+--------------------+\n",
      "|topic1|NULL|-0.16276986724837622|\n",
      "+------+----+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 50\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------+----+--------------------+\n",
      "| topic| key|               value|\n",
      "+------+----+--------------------+\n",
      "|topic1|NULL|-0.16276986724837622|\n",
      "|topic2|NULL|              \"mitt\"|\n",
      "+------+----+--------------------+\n",
      "\n",
      "+------+----+------+\n",
      "| topic| key| value|\n",
      "+------+----+------+\n",
      "|topic2|NULL|\"mitt\"|\n",
      "+------+----+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 106\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 323\n",
      "-------------------------------------------\n",
      "+------+----+--------------------+\n",
      "| topic| key|               value|\n",
      "+------+----+--------------------+\n",
      "|topic1|NULL|-0.16276986724837622|\n",
      "|topic1|NULL| -1.3486250113881726|\n",
      "+------+----+--------------------+\n",
      "\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL|-1.3486250113881726|\n",
      "+------+----+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL|-1.3486250113881726|\n",
      "|topic2|NULL|      \"predeceased\"|\n",
      "+------+----+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 51\n",
      "-------------------------------------------\n",
      "+------+----+-------------+\n",
      "| topic| key|        value|\n",
      "+------+----+-------------+\n",
      "|topic2|NULL|\"predeceased\"|\n",
      "+------+----+-------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 107\n",
      "-------------------------------------------\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL|-0.2940070447689566|\n",
      "+------+----+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 324\n",
      "-------------------------------------------\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL|-0.2940070447689566|\n",
      "+------+----+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 108\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 52\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL|-0.2940070447689566|\n",
      "|topic1|NULL|-1.3543981231171047|\n",
      "+------+----+-------------------+\n",
      "\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL|-1.3543981231171047|\n",
      "+------+----+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 325\n",
      "-------------------------------------------\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL|-1.3543981231171047|\n",
      "+------+----+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+------+----+------------------+\n",
      "| topic| key|             value|\n",
      "+------+----+------------------+\n",
      "|topic1|NULL|-0.826045587301648|\n",
      "|topic2|NULL|          \"annals\"|\n",
      "+------+----+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 109\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 53\n",
      "-------------------------------------------\n",
      "+------+----+------------------+\n",
      "| topic| key|             value|\n",
      "+------+----+------------------+\n",
      "|topic1|NULL|-0.826045587301648|\n",
      "+------+----+------------------+\n",
      "\n",
      "+------+----+--------+\n",
      "| topic| key|   value|\n",
      "+------+----+--------+\n",
      "|topic2|NULL|\"annals\"|\n",
      "+------+----+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 326\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+------+----+------------------+\n",
      "| topic| key|             value|\n",
      "+------+----+------------------+\n",
      "|topic1|NULL|-0.826045587301648|\n",
      "|topic1|NULL|1.4006997579182374|\n",
      "+------+----+------------------+\n",
      "\n",
      "+------+----+------------------+\n",
      "| topic| key|             value|\n",
      "+------+----+------------------+\n",
      "|topic1|NULL|1.4006997579182374|\n",
      "+------+----+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 110\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 54\n",
      "-------------------------------------------\n",
      "+------+----+--------+\n",
      "| topic| key|   value|\n",
      "+------+----+--------+\n",
      "|topic2|NULL|\"vaguer\"|\n",
      "+------+----+--------+\n",
      "\n",
      "+------+----+------------------+\n",
      "| topic| key|             value|\n",
      "+------+----+------------------+\n",
      "|topic1|NULL|1.4006997579182374|\n",
      "+------+----+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+------+----+------------------+\n",
      "| topic| key|             value|\n",
      "+------+----+------------------+\n",
      "|topic1|NULL|0.2351592250048001|\n",
      "|topic2|NULL|          \"vaguer\"|\n",
      "+------+----+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 327\n",
      "-------------------------------------------\n",
      "+------+----+------------------+\n",
      "| topic| key|             value|\n",
      "+------+----+------------------+\n",
      "|topic1|NULL|0.2351592250048001|\n",
      "+------+----+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 111\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 55\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL| 0.2351592250048001|\n",
      "|topic1|NULL|-0.4413045153770099|\n",
      "+------+----+-------------------+\n",
      "\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL|-0.4413045153770099|\n",
      "+------+----+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 328\n",
      "-------------------------------------------\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL|-0.4413045153770099|\n",
      "+------+----+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 56\n",
      "-------------------------------------------\n",
      "+------+----+-------+\n",
      "| topic| key|  value|\n",
      "+------+----+-------+\n",
      "|topic2|NULL|\"Ginny\"|\n",
      "+------+----+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 112\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+------+----+--------------------+\n",
      "| topic| key|               value|\n",
      "+------+----+--------------------+\n",
      "|topic1|NULL|-0.08203546486869032|\n",
      "|topic2|NULL|             \"Ginny\"|\n",
      "+------+----+--------------------+\n",
      "\n",
      "+------+----+--------------------+\n",
      "| topic| key|               value|\n",
      "+------+----+--------------------+\n",
      "|topic1|NULL|-0.08203546486869032|\n",
      "+------+----+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 329\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 57\n",
      "-------------------------------------------\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n",
      "+------+----+--------------------+\n",
      "| topic| key|               value|\n",
      "+------+----+--------------------+\n",
      "|topic1|NULL|-0.08203546486869032|\n",
      "|topic1|NULL|-0.24125447364083902|\n",
      "+------+----+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 113\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+------+----+--------------------+\n",
      "| topic| key|               value|\n",
      "+------+----+--------------------+\n",
      "|topic1|NULL|-0.24125447364083902|\n",
      "+------+----+--------------------+\n",
      "\n",
      "+------+----+--------------------+\n",
      "| topic| key|               value|\n",
      "+------+----+--------------------+\n",
      "|topic1|NULL|-0.24125447364083902|\n",
      "|topic2|NULL|        \"asphalting\"|\n",
      "+------+----+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 58\n",
      "-------------------------------------------\n",
      "+------+----+------------+\n",
      "| topic| key|       value|\n",
      "+------+----+------------+\n",
      "|topic2|NULL|\"asphalting\"|\n",
      "+------+----+------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 330\n",
      "-------------------------------------------\n",
      "+------+----+--------------------+\n",
      "| topic| key|               value|\n",
      "+------+----+--------------------+\n",
      "|topic1|NULL|-0.00180536368735...|\n",
      "+------+----+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 114\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 59\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n",
      "+------+----+--------------------+\n",
      "| topic| key|               value|\n",
      "+------+----+--------------------+\n",
      "|topic1|NULL|-0.00180536368735...|\n",
      "|topic1|NULL|-0.02881352297860...|\n",
      "+------+----+--------------------+\n",
      "\n",
      "+------+----+--------------------+\n",
      "| topic| key|               value|\n",
      "+------+----+--------------------+\n",
      "|topic1|NULL|-0.00180536368735...|\n",
      "|topic1|NULL|-0.02881352297860...|\n",
      "+------+----+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 331\n",
      "-------------------------------------------\n",
      "+------+----+--------------------+\n",
      "| topic| key|               value|\n",
      "+------+----+--------------------+\n",
      "|topic1|NULL|-0.02881352297860...|\n",
      "+------+----+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 60\n",
      "-------------------------------------------\n",
      "+------+----+--------+\n",
      "| topic| key|   value|\n",
      "+------+----+--------+\n",
      "|topic2|NULL|\"Augean\"|\n",
      "+------+----+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 115\n",
      "-------------------------------------------\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL|-0.8230455476049405|\n",
      "|topic2|NULL|           \"Augean\"|\n",
      "+------+----+-------------------+\n",
      "\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL|-0.8230455476049405|\n",
      "+------+----+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 61\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 332\n",
      "-------------------------------------------\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL|-0.8230455476049405|\n",
      "|topic1|NULL|-1.4886456834402277|\n",
      "+------+----+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 116\n",
      "-------------------------------------------\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL|-1.4886456834402277|\n",
      "+------+----+-------------------+\n",
      "\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL|-1.4886456834402277|\n",
      "|topic2|NULL|     \"marshmallows\"|\n",
      "+------+----+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 333\n",
      "-------------------------------------------\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL|-0.2582029921355854|\n",
      "+------+----+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 62\n",
      "-------------------------------------------\n",
      "+------+----+--------------+\n",
      "| topic| key|         value|\n",
      "+------+----+--------------+\n",
      "|topic2|NULL|\"marshmallows\"|\n",
      "+------+----+--------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 334\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 117\n",
      "-------------------------------------------\n",
      "+------+----+------------------+\n",
      "| topic| key|             value|\n",
      "+------+----+------------------+\n",
      "|topic1|NULL|0.7122501086703803|\n",
      "+------+----+------------------+\n",
      "\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL|-0.2582029921355854|\n",
      "|topic1|NULL| 0.7122501086703803|\n",
      "+------+----+-------------------+\n",
      "\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL|-0.2582029921355854|\n",
      "|topic1|NULL| 0.7122501086703803|\n",
      "+------+----+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 63\n",
      "-------------------------------------------\n",
      "+------+----+--------------+\n",
      "| topic| key|         value|\n",
      "+------+----+--------------+\n",
      "|topic2|NULL|\"observance's\"|\n",
      "+------+----+--------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 118\n",
      "-------------------------------------------\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL|-0.7745953571517081|\n",
      "+------+----+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 335\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 15\n",
      "-------------------------------------------\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL|-0.7745953571517081|\n",
      "|topic2|NULL|     \"observance's\"|\n",
      "+------+----+-------------------+\n",
      "\n",
      "+------+----+-------------------+\n",
      "| topic| key|              value|\n",
      "+------+----+-------------------+\n",
      "|topic1|NULL|-0.7745953571517081|\n",
      "+------+----+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 119\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 64\n",
      "-------------------------------------------\n",
      "+------+----+------------------+\n",
      "| topic| key|             value|\n",
      "+------+----+------------------+\n",
      "|topic1|NULL|2.4014574066160432|\n",
      "+------+----+------------------+\n",
      "\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/03 15:45:30 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 336, writer: ConsoleWriter[numRows=20, truncate=true]] is aborting.\n",
      "24/04/03 15:45:30 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 336, writer: ConsoleWriter[numRows=20, truncate=true]] aborted.\n",
      "24/04/03 15:45:30 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: ConsoleWriter[numRows=20, truncate=true]] is aborting.\n",
      "24/04/03 15:45:30 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: ConsoleWriter[numRows=20, truncate=true]] aborted.\n",
      "24/04/03 15:45:30 ERROR MicroBatchExecution: Query [id = 9be15d7f-cd64-4beb-a7a2-32676e539157, runId = 2e0b2fd4-e8b3-42c7-9c35-0de6380b07fa] terminated with error\n",
      "org.apache.spark.SparkException: Job 537 cancelled as part of cancellation of all jobs\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2731)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1114)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1113)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3022)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:359)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:318)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3573)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3573)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/04/03 15:45:30 ERROR MicroBatchExecution: Query [id = 5a42e1ef-772f-4c84-99e8-31c8d8ef53b3, runId = 46cc4911-e22d-4cd6-b4fc-c08cb5b6bd8e] terminated with error\n",
      "org.apache.spark.SparkException: Job 538 cancelled as part of cancellation of all jobs\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2731)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1114)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1113)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3022)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:359)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:318)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3573)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3573)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/04/03 15:45:30 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 120, writer: ConsoleWriter[numRows=20, truncate=true]] is aborting.\n",
      "24/04/03 15:45:30 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 120, writer: ConsoleWriter[numRows=20, truncate=true]] aborted.\n",
      "24/04/03 15:45:30 ERROR MicroBatchExecution: Query [id = b002d82a-8c2a-4bc2-a58d-847fb659a212, runId = 42ed57c7-d3b5-45a8-9d60-6ab095bd5264] terminated with error\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "         \n",
      "java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1659)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1337)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3003)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1595)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1337)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3003)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:359)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:318)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3573)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3573)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1659)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1337)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3003)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "24/04/03 15:45:30 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 65, writer: ConsoleWriter[numRows=20, truncate=true]] is aborting.\n",
      "24/04/03 15:45:30 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 65, writer: ConsoleWriter[numRows=20, truncate=true]] aborted.\n",
      "24/04/03 15:45:30 ERROR MicroBatchExecution: Query [id = 36a9a8f4-b6b1-4b76-842e-585efc17f01f, runId = 7ff09445-c00d-4e05-b028-a7b975710545] terminated with error\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "         \n",
      "java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1659)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1337)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3003)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1595)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1337)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3003)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:359)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:318)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3573)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3573)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1659)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1337)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3003)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "24/04/03 15:45:31 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "24/04/03 15:45:31 ERROR DataWritingSparkTask: Aborting commit for partition 1 (task 827, attempt 0, stage 537.0)\n",
      "24/04/03 15:45:31 ERROR DataWritingSparkTask: Aborted commit for partition 1 (task 827, attempt 0, stage 537.0)\n",
      "24/04/03 15:45:31 ERROR Utils: Uncaught exception in thread Executor task launch worker for task 1.0 in stage 537.0 (TID 827)\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.scheduler.Task.$anonfun$run$3(Task.scala:146)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:144)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "24/04/03 15:45:31 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "24/04/03 15:45:31 ERROR DataWritingSparkTask: Aborting commit for partition 1 (task 829, attempt 0, stage 538.0)\n",
      "24/04/03 15:45:31 ERROR DataWritingSparkTask: Aborted commit for partition 1 (task 829, attempt 0, stage 538.0)\n",
      "24/04/03 15:45:31 ERROR Utils: Uncaught exception in thread Executor task launch worker for task 1.0 in stage 538.0 (TID 829)\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.scheduler.Task.$anonfun$run$3(Task.scala:146)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:144)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# kafka read by streaming\n",
    "# df = spark.readStream.format(\"kafka\")\\\n",
    "#                     .option('kafka.bootstrap.servers','localhost:9092')\\\n",
    "#                     .option('subscribe','topic1,topic2').load()\n",
    "# rawDF = df.selectExpr(\"topic\",\"CAST(key as STRING)\",\"CAST(value as STRING)\")\n",
    "# rawDF\n",
    "kafka_options = {\n",
    "    'kafka.bootstrap.servers':'localhost:9092',\n",
    "    'subscribe':'topic1,topic2'\n",
    "}\n",
    "\n",
    "df = spark.readStream.format(\"kafka\")\\\n",
    "                    .options(**kafka_options).load()\n",
    "rawDF = df.selectExpr(\"topic\",\"CAST(value as STRING)\")\n",
    "rawDF.writeStream.format('console').outputMode('Append').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d698afcf-31aa-47e6-a6ce-1672eae5ae40",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/03 15:45:45 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b54d474d-ecf6-41be-9a1b-7e7ffcb00444. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/03 15:45:45 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/04/03 15:45:46 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------+----+-----------+\n",
      "| topic| key|      value|\n",
      "+------+----+-----------+\n",
      "|topic2|NULL|\"pinnacles\"|\n",
      "|topic2|NULL|\"kiloton's\"|\n",
      "+------+----+-----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------+----+------------+\n",
      "| topic| key|       value|\n",
      "+------+----+------------+\n",
      "|topic2|NULL|\"caliphates\"|\n",
      "|topic2|NULL| \"coating's\"|\n",
      "|topic2|NULL|  \"austerer\"|\n",
      "+------+----+------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+------+----+-------+\n",
      "| topic| key|  value|\n",
      "+------+----+-------+\n",
      "|topic2|NULL|\"Titus\"|\n",
      "+------+----+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+------+----+------+\n",
      "| topic| key| value|\n",
      "+------+----+------+\n",
      "|topic2|NULL|\"hoop\"|\n",
      "+------+----+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+------+----+-------------+\n",
      "| topic| key|        value|\n",
      "+------+----+-------------+\n",
      "|topic2|NULL|\"squirrelled\"|\n",
      "+------+----+-------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+------+----+------+\n",
      "| topic| key| value|\n",
      "+------+----+------+\n",
      "|topic2|NULL|\"jape\"|\n",
      "+------+----+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+------+----+-----------+\n",
      "| topic| key|      value|\n",
      "+------+----+-----------+\n",
      "|topic2|NULL|\"bellyfuls\"|\n",
      "+------+----+-----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+------+----+--------+\n",
      "| topic| key|   value|\n",
      "+------+----+--------+\n",
      "|topic2|NULL|\"teazel\"|\n",
      "+------+----+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+------+----+-------------+\n",
      "| topic| key|        value|\n",
      "+------+----+-------------+\n",
      "|topic2|NULL|\"blackboards\"|\n",
      "+------+----+-------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 15\n",
      "-------------------------------------------\n",
      "+------+----+----------+\n",
      "| topic| key|     value|\n",
      "+------+----+----------+\n",
      "|topic2|NULL|\"campfire\"|\n",
      "+------+----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 16\n",
      "-------------------------------------------\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 17\n",
      "-------------------------------------------\n",
      "+------+----+-----------+\n",
      "| topic| key|      value|\n",
      "+------+----+-----------+\n",
      "|topic2|NULL|\"equipoise\"|\n",
      "+------+----+-----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 18\n",
      "-------------------------------------------\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 19\n",
      "-------------------------------------------\n",
      "+------+----+------------+\n",
      "| topic| key|       value|\n",
      "+------+----+------------+\n",
      "|topic2|NULL|\"idealism's\"|\n",
      "+------+----+------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 20\n",
      "-------------------------------------------\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 21\n",
      "-------------------------------------------\n",
      "+------+----+----------+\n",
      "| topic| key|     value|\n",
      "+------+----+----------+\n",
      "|topic2|NULL|\"apostles\"|\n",
      "+------+----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 22\n",
      "-------------------------------------------\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 23\n",
      "-------------------------------------------\n",
      "+------+----+-----------+\n",
      "| topic| key|      value|\n",
      "+------+----+-----------+\n",
      "|topic2|NULL|\"salivated\"|\n",
      "+------+----+-----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 24\n",
      "-------------------------------------------\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 25\n",
      "-------------------------------------------\n",
      "+------+----+---------+\n",
      "| topic| key|    value|\n",
      "+------+----+---------+\n",
      "|topic2|NULL|\"swathed\"|\n",
      "+------+----+---------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 26\n",
      "-------------------------------------------\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 27\n",
      "-------------------------------------------\n",
      "+------+----+------------+\n",
      "| topic| key|       value|\n",
      "+------+----+------------+\n",
      "|topic2|NULL|\"nonfiction\"|\n",
      "+------+----+------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 28\n",
      "-------------------------------------------\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 29\n",
      "-------------------------------------------\n",
      "+------+----+------------+\n",
      "| topic| key|       value|\n",
      "+------+----+------------+\n",
      "|topic2|NULL|\"unleavened\"|\n",
      "+------+----+------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 30\n",
      "-------------------------------------------\n",
      "+-----+---+-----+\n",
      "|topic|key|value|\n",
      "+-----+---+-----+\n",
      "+-----+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/root/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib64/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 31\n",
      "-------------------------------------------\n",
      "+------+----+---------+\n",
      "| topic| key|    value|\n",
      "+------+----+---------+\n",
      "|topic2|NULL|\"perkier\"|\n",
      "+------+----+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_options = {\n",
    "    'kafka.bootstrap.servers':'localhost:9092',\n",
    "    'subscribe':'topic1,topic2'\n",
    "}\n",
    "\n",
    "df = spark.readStream.format(\"kafka\")\\\n",
    "                    .options(**kafka_options).load()\n",
    "\n",
    "rawDF = df.selectExpr(\"topic\",\"CAST(value as STRING)\")\n",
    "\n",
    "topic1DF = rawDF.filter(col('topic') == 'topic1')\n",
    "topic2DF = rawDF.filter(col('topic') == 'topic2')\n",
    "\n",
    "query = topic2DF.writeStream.format('console').outputMode('Append').start()\n",
    "query.awaitTermination()\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e58e5d-d3ab-461b-a868-1b007285c4fc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/03 16:28:04 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-27f1e6bb-1516-4df9-aee8-83b0d3123723. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/03 16:28:04 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/04/03 16:28:05 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|topic|value|\n",
      "+-----+-----+\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------+--------------------+\n",
      "| topic|               value|\n",
      "+------+--------------------+\n",
      "|topic1| -0.9673703125612044|\n",
      "|topic1| 0.16244183275939456|\n",
      "|topic1| -0.7626922919967754|\n",
      "|topic1|  1.2509249435497942|\n",
      "|topic1|-0.13012370932273243|\n",
      "|topic2|       \"Chayefsky's\"|\n",
      "|topic2|         \"paperback\"|\n",
      "+------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------+--------------------+\n",
      "| topic|               value|\n",
      "+------+--------------------+\n",
      "|topic1| 0.12626473741867397|\n",
      "|topic1| -0.5606113015559223|\n",
      "|topic1|    1.38210651470344|\n",
      "|topic1|  1.1550556908465595|\n",
      "|topic1| -0.5037509802935475|\n",
      "|topic1|-0.15376109458680634|\n",
      "|topic2|   \"homosexuality's\"|\n",
      "|topic2|          \"filliped\"|\n",
      "|topic2|     \"legislature's\"|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/root/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib64/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m rawDF \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(value as STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m query \u001b[38;5;241m=\u001b[39m rawDF\u001b[38;5;241m.\u001b[39mwriteStream\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtrigger(processingTime\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5 seconds\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m----> 9\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+------+--------------------+\n",
      "| topic|               value|\n",
      "+------+--------------------+\n",
      "|topic1|   1.836141420041429|\n",
      "|topic1|  -0.569070685194087|\n",
      "|topic1| -0.7559935585601707|\n",
      "|topic1|-0.20125734837136505|\n",
      "|topic1|-0.10905371484111982|\n",
      "|topic1|  1.8025784646382776|\n",
      "|topic1|  1.3727443876923537|\n",
      "|topic2|             \"elite\"|\n",
      "|topic2|        \"southern's\"|\n",
      "|topic2|              \"Nick\"|\n",
      "|topic2|         \"footstool\"|\n",
      "+------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+------+-------------------+\n",
      "| topic|              value|\n",
      "+------+-------------------+\n",
      "|topic1| 0.4066524160825268|\n",
      "|topic1|-0.6434168654006373|\n",
      "|topic1| 1.8529137767981874|\n",
      "|topic1|0.48133328807168846|\n",
      "|topic1| 0.0563861437402658|\n",
      "|topic1| 0.4924405263026707|\n",
      "|topic1|-0.7492900353075593|\n",
      "|topic1| 1.3783049162361507|\n",
      "|topic1| 1.7672720871271355|\n",
      "|topic1| 0.8053818197326339|\n",
      "|topic2|       \"kneecapped\"|\n",
      "|topic2|           \"colder\"|\n",
      "|topic2|            \"Rio's\"|\n",
      "|topic2|       \"dominion's\"|\n",
      "|topic2|          \"Jules's\"|\n",
      "+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_options = {\n",
    "    'kafka.bootstrap.servers':'localhost:9092',\n",
    "    'subscribe':'topic1,topic2'\n",
    "}\n",
    "df = spark.readStream.format(\"kafka\")\\\n",
    "                    .options(**kafka_options).load()\n",
    "rawDF = df.selectExpr(\"topic\",\"CAST(value as STRING)\")\n",
    "query = rawDF.writeStream.format(\"console\").trigger(processingTime='5 seconds').start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58761f01-0693-4d13-8b05-ae553ccdea79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a2f69a-726d-4a91-84e9-19a90585c59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58d7bd69-b5e6-492f-86ce-15e88bdc8b9a",
   "metadata": {},
   "source": [
    "# 카프카에서 생성한 데이터를 구독해서 일정한 시간 단위로 사용자가 정의한 테이블에 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd29f162-d89e-486f-a2ba-9299a6c9be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, streaming\n",
    "from pyspark.sql.functions import col,expr\n",
    "\n",
    "# create session\n",
    "spark = SparkSession.builder.appName('kafka_spark').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd3c6c57-5226-4e84-a9c3-508356a5375d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DB 저장하기\n",
    "def foreach_batch_function(batch_df,batch_id): \n",
    "    url = \"jdbc:mysql://localhost:3306/hadoopguide?useSSL=false\"\n",
    "    driver='com.mysql.jdbc.Driver'\n",
    "    user='root'\n",
    "    password='password'\n",
    "    tablename = 'tbl_kafka_spark'\n",
    "    # mode = 'overwrite'\n",
    "    mode = 'append'\n",
    "    props = {'driver':driver,'user':user,'password':password}\n",
    "    \n",
    "    params = {\n",
    "        'url': url,\n",
    "        'table' :  tablename,    \n",
    "        'properties' : props,\n",
    "        'mode' : mode\n",
    "    }    \n",
    "    batch_df.write.jdbc(**params)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2f7db6-5557-4dff-a5ce-ab039dcdfae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic, value  table create  직접 만들어 놓기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bab0cb46-c5d3-41ec-afb5-bc391fb5434e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/03 16:34:01 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-90f2e7da-b0d8-4b5e-ade0-a3e9b2f6c006. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/03 16:34:01 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/04/03 16:34:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "ERROR:root:KeyboardInterrupt while sending command.                             \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/root/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib64/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m      7\u001b[0m rawDF \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(value as STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m query \u001b[38;5;241m=\u001b[39m rawDF\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;241m.\u001b[39mtrigger(processingTime\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5 seconds\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;241m.\u001b[39mforeachBatch(foreach_batch_function) \\\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;241m.\u001b[39mstart()  \n\u001b[0;32m---> 14\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kafka_options = {\n",
    "    'kafka.bootstrap.servers':'localhost:9092',\n",
    "    'subscribe':'topic1,topic2'\n",
    "}\n",
    "df = spark.readStream.format(\"kafka\")\\\n",
    "                    .options(**kafka_options).load()\n",
    "rawDF = df.selectExpr(\"topic\",\"CAST(value as STRING)\")\n",
    "\n",
    "query = rawDF.writeStream \\\n",
    "        .format(\"console\") \\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .foreachBatch(foreach_batch_function) \\\n",
    "        .start()  \n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417199e9-e615-43e0-a1af-3dce6c940bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
