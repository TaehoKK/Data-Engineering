{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e4de64b6-858f-4fe9-8f3f-524c314a5f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "person = spark.createDataFrame([\n",
    "    (0, \"Bill Chambers\", 0, [100]),\n",
    "    (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "    (2, \"Michael Armbrust\", 1, [250, 100])])\\\n",
    "  .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "graduateProgram = spark.createDataFrame([\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n",
    "  .toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "sparkStatus = spark.createDataFrame([\n",
    "    (500, \"Vice President\"),\n",
    "    (250, \"PMC Member\"),\n",
    "    (100, \"Contributor\")])\\\n",
    "  .toDF(\"id\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e0132fe3-ba7d-48dc-9851-58a7f0d86a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinExpresion = person['graduate_program'] == graduateProgram['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "174112cc-12fa-443e-8615-0fa542c8eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinType = 'inner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5958d4a3-ce16-438a-9312-13bd89bf4196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:>                 (0 + 4) / 4][Stage 74:>                 (0 + 0) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "person.join(graduateProgram,joinExpresion,joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7fee40e0-95d3-4ffa-a6f3-c3aad9bf7fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "graduateProgram2 = graduateProgram.union(spark.createDataFrame([\n",
    "    (0,'Master',\"Duplicated Row\",\"Duplicated School\")\n",
    "]))\n",
    "graduateProgram2.createOrReplaceTempView('graduateProgram2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "faa47529-39a9-4f27-a654-b8d387127184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------------+\n",
      "| id| degree|          department|           school|\n",
      "+---+-------+--------------------+-----------------+\n",
      "|  0|Masters|School of Informa...|      UC Berkeley|\n",
      "|  2|Masters|                EECS|      UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|      UC Berkeley|\n",
      "|  0| Master|      Duplicated Row|Duplicated School|\n",
      "+---+-------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# graduateProgram2 내용 확인하기\n",
    "graduateProgram2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "abfde109-f1bb-4957-9556-4771c02c9ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------------+\n",
      "| id| degree|          department|           school|\n",
      "+---+-------+--------------------+-----------------+\n",
      "|  0|Masters|School of Informa...|      UC Berkeley|\n",
      "|  2|Masters|                EECS|      UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|      UC Berkeley|\n",
      "|  0| Master|      Duplicated Row|Duplicated School|\n",
      "+---+-------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select * from graduateProgram2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5638ef7f-80c8-44b1-8768-3797449f2da0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|        status|\n",
      "+---+--------------+\n",
      "|500|Vice President|\n",
      "|250|    PMC Member|\n",
      "+---+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkStatus.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b4aa5800-d6af-448a-8630-88a6417bdab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+----------------+---------------+---+--------------+\n",
      "|persionid|            name|graduate_program|   spark_status| id|        status|\n",
      "+---------+----------------+----------------+---------------+---+--------------+\n",
      "|        0|   Bill Chambers|               0|          [100]|100|   Contributor|\n",
      "|        1|   Matei Zaharia|               1|[500, 250, 100]|500|Vice President|\n",
      "|        1|   Matei Zaharia|               1|[500, 250, 100]|250|    PMC Member|\n",
      "|        1|   Matei Zaharia|               1|[500, 250, 100]|100|   Contributor|\n",
      "|        2|Michael Armbrust|               1|     [250, 100]|250|    PMC Member|\n",
      "|        2|Michael Armbrust|               1|     [250, 100]|100|   Contributor|\n",
      "+---------+----------------+----------------+---------------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "person.withColumnRenamed('id','persionid').join(sparkStatus,\n",
    "                                                expr(\"array_contains(spark_status,id)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1a7db123-60d7-470d-aba0-491e12e50212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2010-summary.csv\n",
    "# .option('mode','FAILEFAST') 잘못된 레코드를 발견하면 즉시 반환\n",
    "csvFile = spark.read.format('csv').option('header','true').option('inferSchema','true')\\\n",
    ".option('mode','FAILFAST').load('2015-summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "515c8b71-8797-4393-b8e8-4aa50bf8142c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvFile.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0a68f8c0-2c83-4322-8003-9855b7065dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format('csv').mode('overwrite').option('sep','\\t')\\\n",
    ".save('my-file.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "30a91537-010d-443d-af95-05671b2718e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile2 = spark.read.format('csv').option('header','true').option('inferSchema','true')\\\n",
    ".option('mode','FAILFAST').option('sep','\\t').load('my-file.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cd34feea-91b9-4ebd-a9c9-3e5b3735e8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+---+\n",
      "|       United States|         Romania| 15|\n",
      "+--------------------+----------------+---+\n",
      "|       United States|         Croatia|  1|\n",
      "|       United States|         Ireland|344|\n",
      "|               Egypt|   United States| 15|\n",
      "|       United States|           India| 62|\n",
      "|       United States|       Singapore|  1|\n",
      "|       United States|         Grenada| 62|\n",
      "|          Costa Rica|   United States|588|\n",
      "|             Senegal|   United States| 40|\n",
      "|             Moldova|   United States|  1|\n",
      "|       United States|    Sint Maarten|325|\n",
      "|       United States|Marshall Islands| 39|\n",
      "|              Guyana|   United States| 64|\n",
      "|               Malta|   United States|  1|\n",
      "|            Anguilla|   United States| 41|\n",
      "|             Bolivia|   United States| 30|\n",
      "|       United States|        Paraguay|  6|\n",
      "|             Algeria|   United States|  4|\n",
      "|Turks and Caicos ...|   United States|230|\n",
      "|       United States|       Gibraltar|  1|\n",
      "|Saint Vincent and...|   United States|  1|\n",
      "+--------------------+----------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvFile2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "057ad5c4-1952-4266-b616-e1800acd755d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# json read show\n",
    "spark.read.format('json').option('inferSchema','true').option('mode','FAILFAST')\\\n",
    ".load('2015-summary.json').show(3)\n",
    "# csvFile -- json write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "696ab8c3-586d-4c47-9ccb-e255e2e11e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format('json').mode('overwrite').save('my-json-file.json');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e19e10da-db7e-4beb-91c1-a46d364ad7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('json').option('inferSchema','true').option('mode','FAILFAST')\\\n",
    ".load('my-json-file.json').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9446ec69-db29-4206-97bf-25a93ead6104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet 분석용 데이터를 저장하는데 사용되는 파일 형기\n",
    "# 압축 / 분할 저장 / 스키마 지원 / 서드파티 지원 / 열 지향 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "259fa594-27c8-4a1a-aec5-0c2f2a7e2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# spark2 = SparkSession.builder.appName('Create Parquet File').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "439c1133-b7e2-487f-a15f-b7cc91d112c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    ('a',10),('b',20),('c',30)\n",
    "]\n",
    "df = spark.createDataFrame(data,['name','age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f1917e94-3744-4c32-ad40-e6ce964a5980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.mode('overwrite').parquet('output.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0719ae27-6890-4c47-ac02-e9a0d400b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0137bf6e-27f4-4db4-9113-0b8185b22ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|   b| 20|\n",
      "|   c| 30|\n",
      "|   a| 10|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('parquet').load('output.parquet').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cdb9bc40-c8c8-4d2c-80ad-b34b77164bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvFile.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8736b5da-b34e-40e6-a643-5fd226929c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format('parquet').mode('overwrite').save('csvFile-parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e80136c2-a6ea-428d-89c8-ae364748870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORC(Optimized Row Columnar) file ?\n",
    "# 대규모 데이터 저장 및 처리 hive 에서 분산처리할때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4b6d7d67-ef74-4233-a0ae-e10890bb7555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('orc').load('2010-summary.orc').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ed48dcb2-3ff8-4982-88ab-14e7b4d44706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORC save\n",
    "csvFile.write.format('orc').mode('overwrite').save('my-orc-file.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1cdac60f-092d-45e0-9616-2b5d90a3f363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('orc').load('my-orc-file.orc').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2c5e96ad-66ef-42c3-93a7-565ee57916a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2eb2134b-ca89-4349-8266-57a16fc8f97e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mysql_spark = SparkSession.builder.config('spark.jars', \"mysql-connector-java-5.1.46.jar\")\\\n",
    ".master('local').appName('pySpark_MySql').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b7fb686c-ae01-4de1-b137-4c32a497a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    mysql_spark.read.format('jdbc')\n",
    "    .option(\"url\",'jdbc:mysql://localhost:3306/hadoopguide?useSSL=false')\n",
    "    .option('driver','com.mysql.jdbc.Driver')\n",
    "    .option('dbtable','tbl_target')\n",
    "    .option('user','root').option('password','password')\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "40b60d49-7578-4b7f-bd73-577e8f885cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|testno|testname|\n",
      "+------+--------+\n",
      "|     4|   44444|\n",
      "|     5|   55555|\n",
      "|     1|   11111|\n",
      "|     2|   22222|\n",
      "|     3|   33333|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "708ee1ea-2c27-4fd4-86ca-0bb8a4a430cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = \"com.mysql.jdbc.Driver\"\n",
    "url = \"jdbc:mysql://localhost:3306/hadoopguide?useSSL=false\"\n",
    "tablename = \"tbl_target\"\n",
    "user = 'root'\n",
    "password = 'password'\n",
    "\n",
    "dbDataFrame = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"dbtable\", tablename)\\\n",
    "    .option(\"driver\",  driver)\\\n",
    "    .option(\"user\",user)\\\n",
    "    .option('password',password)\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a35a522f-efd2-4aa5-8b8b-1e5235b745c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|testno|testname|\n",
      "+------+--------+\n",
      "|     4|   44444|\n",
      "|     5|   55555|\n",
      "|     1|   11111|\n",
      "|     2|   22222|\n",
      "|     3|   33333|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3066f13d-0310-4cf7-9b73-4498f2387df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|testno|testname|\n",
      "+------+--------+\n",
      "|     5|   55555|\n",
      "|     1|   11111|\n",
      "|     3|   33333|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_query = '''\n",
    "    (select * from tbl_target where testno in (1,3,5))\n",
    "    as myinfo\n",
    "'''\n",
    "tablename = sql_query\n",
    "dbDataFrame = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"driver\",  driver)\\\n",
    "    .option(\"user\",user)\\\n",
    "    .option('password',password)\\\n",
    "    .option(\"dbtable\", tablename)\\\n",
    "    .load()\n",
    "dbDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "54c365fa-0dc8-4297-be0e-0e66f656d0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|testno|testname|\n",
      "+------+--------+\n",
      "|     4|   44444|\n",
      "|     5|   55555|\n",
      "|     1|   11111|\n",
      "|     2|   22222|\n",
      "|     3|   33333|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tablename = 'tbl_target'\n",
    "dbDataFrame = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"driver\",  driver)\\\n",
    "    .option(\"user\",user)\\\n",
    "    .option('password',password)\\\n",
    "    .option(\"dbtable\", tablename)\\\n",
    "    .option('numPartitions',10)\\\n",
    "    .load()\n",
    "dbDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8c8b1aea-cec9-4768-9397-23acd54dc542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|testno|testname|\n",
      "+------+--------+\n",
      "|     1|   11111|\n",
      "|     2|   22222|\n",
      "|     3|   33333|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testno !=4 and testname != 55555\n",
    "predicates = [\n",
    "    \"testno != 4 and testname != '55555'\"    \n",
    "]\n",
    "props = {'driver':driver,'user':user,'password':password}\n",
    "spark.read.jdbc(url,tablename,predicates=predicates, properties=props).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9977abb7-6bd7-45e5-a220-9c9386951ba5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method jdbc in module pyspark.sql.readwriter:\n",
      "\n",
      "jdbc(url: str, table: str, column: Optional[str] = None, lowerBound: Union[str, int, NoneType] = None, upperBound: Union[str, int, NoneType] = None, numPartitions: Optional[int] = None, predicates: Optional[List[str]] = None, properties: Optional[Dict[str, str]] = None) -> 'DataFrame' method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Construct a :class:`DataFrame` representing the database table named ``table``\n",
      "    accessible via JDBC URL ``url`` and connection ``properties``.\n",
      "    \n",
      "    Partitions of the table will be retrieved in parallel if either ``column`` or\n",
      "    ``predicates`` is specified. ``lowerBound``, ``upperBound`` and ``numPartitions``\n",
      "    is needed when ``column`` is specified.\n",
      "    \n",
      "    If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    table : str\n",
      "        the name of the table\n",
      "    column : str, optional\n",
      "        alias of ``partitionColumn`` option. Refer to ``partitionColumn`` in\n",
      "        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      "        for the version you use.\n",
      "    predicates : list, optional\n",
      "        a list of expressions suitable for inclusion in WHERE clauses;\n",
      "        each one defines one partition of the :class:`DataFrame`\n",
      "    properties : dict, optional\n",
      "        a dictionary of JDBC database connection arguments. Normally at\n",
      "        least properties \"user\" and \"password\" with their corresponding values.\n",
      "        For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      "    \n",
      "    Other Parameters\n",
      "    ----------------\n",
      "    Extra options\n",
      "        For the extra options, refer to\n",
      "        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      "        for the version you use.\n",
      "    \n",
      "        .. # noqa\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Don't create too many partitions in parallel on a large cluster;\n",
      "    otherwise Spark might crash your external database systems.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.jdbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dc14e2c3-50a6-4a72-8478-4b3041d94e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.jdbc(url,tablename,predicates=predicates, properties=props)\\\n",
    ".rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "115a1a23-126f-4d54-a0ae-e46cfbd1aa61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicates = [\n",
    "    \"testno != 4 and testname != '55555'\"    \n",
    "]\n",
    "props = {'driver':driver,'user':user,'password':password}\n",
    "spark.read.jdbc(url,tablename,predicates=predicates, properties=props).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "41f2cefd-1fa9-4a18-b4c6-f853b88bb9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3\n",
      "1 2 3\n",
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "def test(a,b,c):\n",
    "    print(a,b,c)\n",
    "\n",
    "params = {\n",
    "    'a':1, 'b':2, 'c':3\n",
    "}\n",
    "test(1,2,3)\n",
    "\n",
    "test(**params)\n",
    "\n",
    "params2 = [1,2,3]\n",
    "test(*params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "38e4ea90-4f9a-4083-82bb-fb79cd16b04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|testno|testname|\n",
      "+------+--------+\n",
      "|     1|   11111|\n",
      "|     2|   22222|\n",
      "|     3|   33333|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicates = [\n",
    "    \"testno != 4 and testname != '55555'\"    \n",
    "]\n",
    "props = {'driver':driver,'user':user,'password':password}\n",
    "\n",
    "params = {\n",
    "    'url': url,\n",
    "    'table' :  tablename,\n",
    "    'predicates' : predicates,\n",
    "    'properties' : props    \n",
    "}\n",
    "\n",
    "spark.read.jdbc(**params).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "63cc19fd-c408-49cc-baa0-fcc554a4bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql table read.... 다양하게 출력\n",
    "# schema : mysql    table : help_keyword\n",
    "# 1 read, variable select......\n",
    "\n",
    "# mysql table read----- [format.... jdbc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8109b3c2-3943-493d-a817-95a1f3864f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "props = {'driver':driver,'user':user,'password':password}\n",
    "\n",
    "url = \"jdbc:mysql://localhost:3306/mysql?useSSL=false\"\n",
    "tablename = 'help_keyword'\n",
    "params = {\n",
    "    'url': url,\n",
    "    'table' :  tablename,    \n",
    "    'properties' : props    \n",
    "}\n",
    "\n",
    "help_keyword_df = spark.read.jdbc(**params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f5e97880-0f4d-4f49-8f41-c748f54f3f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "help_keyword_df.createOrReplaceTempView('help_keyword')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6250f3bd-03d4-4255-a763-09d80b30d644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|help_keyword_id|                name|\n",
      "+---------------+--------------------+\n",
      "|            108|%                ...|\n",
      "|            264|&                ...|\n",
      "|            422|(JSON            ...|\n",
      "|             86|*                ...|\n",
      "|             84|+                ...|\n",
      "+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select * from help_keyword limit 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "116443a0-1c2f-49ac-bc0c-230041344316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1728"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = 'bydata/by-day/2011-10-23.csv'\n",
    "df = spark.read.format('csv').option(\"header\",\"true\").option('inferSchema','true').load(filepath)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9e2b459e-8bfe-4599-ad75-f9082a2e89b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method jdbc in module pyspark.sql.readwriter:\n",
      "\n",
      "jdbc(url: str, table: str, mode: Optional[str] = None, properties: Optional[Dict[str, str]] = None) -> None method of pyspark.sql.readwriter.DataFrameWriter instance\n",
      "    Saves the content of the :class:`DataFrame` to an external database table via JDBC.\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    table : str\n",
      "        Name of the table in the external database.\n",
      "    mode : str, optional\n",
      "        specifies the behavior of the save operation when data already exists.\n",
      "    \n",
      "        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "        * ``overwrite``: Overwrite existing data.\n",
      "        * ``ignore``: Silently ignore this operation if data already exists.\n",
      "        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      "    properties : dict\n",
      "        a dictionary of JDBC database connection arguments. Normally at\n",
      "        least properties \"user\" and \"password\" with their corresponding values.\n",
      "        For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      "    \n",
      "    Other Parameters\n",
      "    ----------------\n",
      "    Extra options\n",
      "        For the extra options, refer to\n",
      "        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      "        for the version you use.\n",
      "    \n",
      "        .. # noqa\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Don't create too many partitions in parallel on a large cluster;\n",
      "    otherwise Spark might crash your external database systems.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.write.jdbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "131b19db-8136-4643-a9a8-1880eeb6bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "props = {'driver':driver,'user':user,'password':password}\n",
    "\n",
    "url = \"jdbc:mysql://localhost:3306/hadoopguide?useSSL=false\"\n",
    "tablename = 'flight_data'\n",
    "mode = 'overwrite'\n",
    "params = {\n",
    "    'url': url,\n",
    "    'table' :  tablename,    \n",
    "    'properties' : props,\n",
    "    'mode' : mode\n",
    "}\n",
    "\n",
    "df.write.jdbc(**params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "f10fca31-f237-4d77-a381-e8db4776d8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = 'bydata/by-day/*.csv'\n",
    "df = spark.read.format('csv').option(\"header\",\"true\").option('inferSchema','true').load(filepath)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f7b51091-7de2-482a-89eb-daeaae7611fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "props = {'driver':driver,'user':user,'password':password}\n",
    "\n",
    "url = \"jdbc:mysql://localhost:3306/hadoopguide?useSSL=false\"\n",
    "tablename = 'flight_data'\n",
    "mode = 'overwrite'\n",
    "partitionColumn = 'InvoiceDate'\n",
    "params = {\n",
    "    'url': url,\n",
    "    'table' :  tablename,    \n",
    "    'properties' : props,\n",
    "    'mode' : mode,\n",
    "    # 'partitionColumn':partitionColumn,\n",
    "    # 'lowerBound' : 0, \n",
    "    # 'upperBound' : 10\n",
    "}\n",
    "\n",
    "df.write.jdbc(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "92e5b0c2-e6f6-4b2e-937a-faa18f10f618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1b5d76e5-3672-4d72-8ec0-7297e4f74471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|       Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "+---------+---------+------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CRUD\n",
    "# Read - select\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "daaa9a31-3f51-4b56-8e55-27e5529ed367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read   Quantity >= 100  df_over_100\n",
    "predicates = [\n",
    "    \"Quantity >= 100\"    \n",
    "]\n",
    "props = {'driver':driver,'user':user,'password':password}\n",
    "\n",
    "params = {\n",
    "    'url': url,\n",
    "    'table' :  tablename,\n",
    "    'predicates' : predicates,\n",
    "    'properties' : props    \n",
    "}\n",
    "\n",
    "df_over_100 = spark.read.jdbc(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "bcbe94ad-cfed-405a-9156-251f6fcea125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+\n",
      "|   571937|    16045|POPART WOODEN PEN...|     100|2011-10-20 09:28:00|     0.04|   14911.0|   EIRE|\n",
      "|   571937|    22161|HEART DECORATION ...|     240|2011-10-20 09:28:00|     0.19|   14911.0|   EIRE|\n",
      "|   571937|    22266|EASTER DECORATION...|     640|2011-10-20 09:28:00|     0.19|   14911.0|   EIRE|\n",
      "|   571937|    22267|EASTER DECORATION...|     120|2011-10-20 09:28:00|     0.39|   14911.0|   EIRE|\n",
      "|   571937|    22286|DECORATION WOBBLY...|     120|2011-10-20 09:28:00|     0.39|   14911.0|   EIRE|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_over_100.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "da03f9de-fe51-46c5-87f0-c269385c4239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update -- read data and update and overwrite\n",
    "# not recomment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "346243d6-a623-4e5a-a0f1-fd3c467232a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props = {'driver':driver,'user':user,'password':password}\n",
    "\n",
    "url = \"jdbc:mysql://localhost:3306/hadoopguide?useSSL=false\"\n",
    "tablename = 'flight_data'\n",
    "params = {\n",
    "    'url': url,\n",
    "    'table' :  tablename,    \n",
    "    'properties' : props,    \n",
    "}\n",
    "\n",
    "flight_df = spark.read.jdbc(**params)\n",
    "flight_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8bb82cde-e756-44c3-b3a4-f929def945f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|        Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+-------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084| RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|    179.0|   14075.0|United Kingdom|\n",
      "|   580538|    23077|DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|    125.0|   14075.0|United Kingdom|\n",
      "+---------+---------+-------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark dataFrame  immutable .... 불변\n",
    "from pyspark.sql.functions import col\n",
    "update_df = flight_df.withColumn('UnitPrice',col('UnitPrice')*100)\n",
    "update_df.show(2)\n",
    "update_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3a3554da-ba4f-4a45-8e3e-15fe92608981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'jdbc:mysql://localhost:3306/hadoopguide?useSSL=false', 'table': 'flight_data', 'properties': {'driver': 'com.mysql.jdbc.Driver', 'user': 'root', 'password': 'password'}, 'mode': 'append'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "[item for item in params.items()]\n",
    "params['mode'] = 'append'\n",
    "params['table'] = 'flight_data'\n",
    "print(params)\n",
    "update_df.write.jdbc(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6ba48494-2c5a-487f-b9a9-381b978f0597",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_df_copy = update_df.select('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "13fc1f30-2bca-4327-adf2-13b9280199b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(541909, 541909)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_df_copy.count(), update_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "411a66f2-3e06-4839-b417-efccc2fef5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "update_df_copy.write.jdbc(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b5d18cba-2172-4770-be40-d02b06c16918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 247:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|        Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+-------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084| RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|    179.0|   14075.0|United Kingdom|\n",
      "|   580538|    23077|DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|    125.0|   14075.0|United Kingdom|\n",
      "+---------+---------+-------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "update_df_copy.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0e9f6982-6511-4306-90d8-0dd91feb30df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'jdbc:mysql://localhost:3306/hadoopguide?useSSL=false', 'table': 'flight_data', 'properties': {'driver': 'com.mysql.jdbc.Driver', 'user': 'root', 'password': 'password'}, 'mode': 'overwrite'}\n"
     ]
    }
   ],
   "source": [
    "[item for item in params.items()]\n",
    "params['mode'] = 'overwrite'\n",
    "params['table'] = 'flight_data'\n",
    "print(params)\n",
    "update_df_copy.write.jdbc(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd1decf-80d8-4d0c-be00-12e33b13d132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
