{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe633eeb-21ff-4ec6-b019-e66b4d8ca5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_data = \"Spark The Definitive Guid : Big Data Processing Mode Simple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d35def2-5eeb-4549-a863-e34814d41f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "supplementData = {\n",
    "    \"Spark\":1000, 'Definitive':200,'Big':300,'Simple':100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1e1f841-6653-4fa9-a89f-c9d1c61a4242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcast : 클러스터의 모든 노드에 효율적으로 공유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6480a5c-b286-49c0-a263-e859e3302f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "suppBroadcast =  spark.sparkContext.broadcast(supplementData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fc6180c-bfde-4db2-8753-87b1d3c4b57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Spark': 1000, 'Definitive': 200, 'Big': 300, 'Simple': 100}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suppBroadcast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb8a67bc-5e41-468b-88f0-db26d1c82204",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark',\n",
       " 'The',\n",
       " 'Definitive',\n",
       " 'Guid',\n",
       " ':',\n",
       " 'Big',\n",
       " 'Data',\n",
       " 'Processing',\n",
       " 'Mode',\n",
       " 'Simple']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = spark.sparkContext.parallelize(collection_data.split(),2)  # RDD paprtitioin-->2\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56772125-a1c7-43e4-b9ab-6357e2ba5995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suppBroadcast.value.get('Definitive',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7e840df-6f3e-495b-ac1e-9a160ab86dd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 1000),\n",
       " ('Big', 300),\n",
       " ('Definitive', 200),\n",
       " ('Simple', 100),\n",
       " ('The', 0),\n",
       " ('Guid', 0),\n",
       " (':', 0),\n",
       " ('Data', 0),\n",
       " ('Processing', 0),\n",
       " ('Mode', 0)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(lambda word : (word,suppBroadcast.value.get(word,0))).sortBy(lambda x:x[1], ascending=False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f91f4089-c4ba-47da-bd73-6295743824c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc52bb33-9080-4985-9866-0bc73f157f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight = spark.read.parquet(\"summary.parquet\")\n",
    "flight.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01f3c3a9-2117-42ac-9a1e-b2fdbffda89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 항공편추적하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfdf4bc3-c9e3-4d3d-981c-1fa048d3c482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우리나라 항공편 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14345aac-22b3-4ce7-a1fb-c608b1404a4c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   DEST_COUNTRY_NAME|\n",
      "+--------------------+\n",
      "|            Anguilla|\n",
      "|              Russia|\n",
      "|            Paraguay|\n",
      "|             Senegal|\n",
      "|              Sweden|\n",
      "|            Kiribati|\n",
      "|              Guyana|\n",
      "|         Philippines|\n",
      "|            Malaysia|\n",
      "|           Singapore|\n",
      "|                Fiji|\n",
      "|              Turkey|\n",
      "|             Germany|\n",
      "|         Afghanistan|\n",
      "|              Jordan|\n",
      "|               Palau|\n",
      "|Turks and Caicos ...|\n",
      "|              France|\n",
      "|              Greece|\n",
      "|              Taiwan|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight.select('DEST_COUNTRY_NAME').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09e29f4e-2217-4ab7-a9a6-67ce906185c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|        South Korea|  621|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,lower\n",
    "flight.filter( lower(col('ORIGIN_COUNTRY_NAME')).like(\"%kor%\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17d25433-5a80-48ae-92b7-b5b92b9ba472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|      South Korea|      United States|  683|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight.filter( lower(col('DEST_COUNTRY_NAME')).like(\"%kor%\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e0f2aca-d285-4a3a-886d-7904a29292c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulator 분산환경에서 공유되는 변수 - 여러작업에서 공통적으로 사용되는 값을 수집\n",
    "# 읽기 전용\n",
    "# 클러스터의 각 노드에 값을 추가 , 로컬에서는 변경할수 없다\n",
    "# 디버깅용 정보수집이나 로그작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bdcc4319-0b6d-4f40-8221-5c0bf28fd171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accKor = spark.sparkContext\n",
    "accKor = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2a1c43d-1c81-4d28-8ce0-87f5efe88b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accKorFunc(frow):\n",
    "    dest = frow['DEST_COUNTRY_NAME']\n",
    "    origin = frow['ORIGIN_COUNTRY_NAME']\n",
    "    if (dest == 'South Korea') or  (origin == 'South Korea'):\n",
    "        accKor.add(frow['count'])\n",
    "\n",
    "flight.foreach(lambda frow : accKorFunc(frow))  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "02d83ee9-d56a-40e4-838a-206bd6fa5224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1304"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accKor.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb1aee3a-215b-44aa-89db-626c7e517ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신규 세션 생성하기\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2042339d-18c7-4e1b-861b-d77fd6d25326",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_spark = SparkSession.builder.appName(\"new spark session\").master(\"local[*]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "99db74ac-dc36-46c8-944e-6ef983f9815b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession.Builder at 0x7f21b91fa490>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "611a622d-fc96-4456-beee-beabce62f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3ac24b9c-9441-4b2d-996f-45f2f8bd19d2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-summary.csv                    \u001b[0m\u001b[01;34mretail-data\u001b[0m/\n",
      "\u001b[01;34mIdeaProjects\u001b[0m/                       root\n",
      "README.md                           sample_libsvm_data.txt\n",
      "\u001b[01;34mactivity-data\u001b[0m/                      sample_movielens_ratings.txt\n",
      "anaconda-ks.cfg                     \u001b[01;34msimple-ml\u001b[0m/\n",
      "\u001b[01;34mapache-hive\u001b[0m/                        \u001b[01;34msimple-ml-integers\u001b[0m/\n",
      "\u001b[01;34mbike-data\u001b[0m/                          \u001b[01;34msimple-ml-scaling\u001b[0m/\n",
      "\u001b[01;34mbinary-classification\u001b[0m/              spar_module_ml.ipynb\n",
      "\u001b[01;34mclustering\u001b[0m/                         \u001b[01;34mspark\u001b[0m/\n",
      "\u001b[01;31mdata.zip\u001b[0m                            \u001b[01;34mspark-3.5.1-bin-hadoop3\u001b[0m/\n",
      "\u001b[01;34mdeep-learning-images\u001b[0m/               \u001b[01;31mspark-3.5.1-bin-hadoop3.tgz\u001b[0m\n",
      "default.csv                         spark-3.5.1-bin-hadoop3.tgz.1\n",
      "derby.log                           spark-join.ipynb\n",
      "doc_log.csv                         spark_share_variable.ipynb\n",
      "encoded_test.csv                    spart\n",
      "encoded_test2.csv                   spartk_RDD.ipynb\n",
      "\u001b[01;34mflight-data\u001b[0m/                        \u001b[01;34msqoop\u001b[0m/\n",
      "\u001b[01;34mflight-data-hive\u001b[0m/                   \u001b[01;31msqoop-1.4.7.bin__hadoop-2.6.0.tar.gz\u001b[0m\n",
      "\u001b[01;34mhadoop\u001b[0m/                             summary.parquet\n",
      "\u001b[01;34midea-IC-233.14475.28\u001b[0m/               \u001b[01;34mtest\u001b[0m/\n",
      "\u001b[01;34mjdk\u001b[0m/                                test3.csv\n",
      "merged_file.txt                     \u001b[01;34m공개\u001b[0m/\n",
      "\u001b[01;34mmetastore_db\u001b[0m/                       \u001b[01;34m다운로드\u001b[0m/\n",
      "\u001b[01;34mmulticlass-classification\u001b[0m/          \u001b[01;34m문서\u001b[0m/\n",
      "\u001b[01;34mmy-file.csv\u001b[0m/                        \u001b[01;34m바탕화면\u001b[0m/\n",
      "\u001b[01;34mmysql-connector-java-5.1.46\u001b[0m/        \u001b[01;34m비디오\u001b[0m/\n",
      "\u001b[01;31mmysql-connector-java-5.1.46.tar.gz\u001b[0m  \u001b[01;34m사진\u001b[0m/\n",
      "online-dataset.csv                  \u001b[01;34m서식\u001b[0m/\n",
      "\u001b[01;34mregression\u001b[0m/                         \u001b[01;34m음악\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8998b4ac-abc3-4101-bbf1-a57df6641006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|is_glass| count|\n",
      "+--------+------+\n",
      "|    NULL|  1454|\n",
      "|    true| 12861|\n",
      "|   false|527594|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.options(header='true',inferSchema='true').csv('online-dataset.csv').repartition(2)\\\n",
    ".selectExpr(\"instr(Description , 'GLASS') >=1 as is_glass\")\\\n",
    ".groupBy('is_glass').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0bc0cc1e-e14d-4098-850f-69b2e0885a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cashing..... 성능 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "843f6188-04ea-4b05-be64-39468bb7ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame no cashing origin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d18424c-4f88-4814-b7e2-b8b914774d06",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-12-01.csv  2011-02-21.csv  2011-05-09.csv  2011-07-20.csv  2011-09-30.csv\n",
      "2010-12-02.csv  2011-02-22.csv  2011-05-10.csv  2011-07-21.csv  2011-10-02.csv\n",
      "2010-12-03.csv  2011-02-23.csv  2011-05-11.csv  2011-07-22.csv  2011-10-03.csv\n",
      "2010-12-05.csv  2011-02-24.csv  2011-05-12.csv  2011-07-24.csv  2011-10-04.csv\n",
      "2010-12-06.csv  2011-02-25.csv  2011-05-13.csv  2011-07-25.csv  2011-10-05.csv\n",
      "2010-12-07.csv  2011-02-27.csv  2011-05-15.csv  2011-07-26.csv  2011-10-06.csv\n",
      "2010-12-08.csv  2011-02-28.csv  2011-05-16.csv  2011-07-27.csv  2011-10-07.csv\n",
      "2010-12-09.csv  2011-03-01.csv  2011-05-17.csv  2011-07-28.csv  2011-10-09.csv\n",
      "2010-12-10.csv  2011-03-02.csv  2011-05-18.csv  2011-07-29.csv  2011-10-10.csv\n",
      "2010-12-12.csv  2011-03-03.csv  2011-05-19.csv  2011-07-31.csv  2011-10-11.csv\n",
      "2010-12-13.csv  2011-03-04.csv  2011-05-20.csv  2011-08-01.csv  2011-10-12.csv\n",
      "2010-12-14.csv  2011-03-06.csv  2011-05-22.csv  2011-08-02.csv  2011-10-13.csv\n",
      "2010-12-15.csv  2011-03-07.csv  2011-05-23.csv  2011-08-03.csv  2011-10-14.csv\n",
      "2010-12-16.csv  2011-03-08.csv  2011-05-24.csv  2011-08-04.csv  2011-10-16.csv\n",
      "2010-12-17.csv  2011-03-09.csv  2011-05-25.csv  2011-08-05.csv  2011-10-17.csv\n",
      "2010-12-19.csv  2011-03-10.csv  2011-05-26.csv  2011-08-07.csv  2011-10-18.csv\n",
      "2010-12-20.csv  2011-03-11.csv  2011-05-27.csv  2011-08-08.csv  2011-10-19.csv\n",
      "2010-12-21.csv  2011-03-13.csv  2011-05-29.csv  2011-08-09.csv  2011-10-20.csv\n",
      "2010-12-22.csv  2011-03-14.csv  2011-05-31.csv  2011-08-10.csv  2011-10-21.csv\n",
      "2010-12-23.csv  2011-03-15.csv  2011-06-01.csv  2011-08-11.csv  2011-10-23.csv\n",
      "2011-01-04.csv  2011-03-16.csv  2011-06-02.csv  2011-08-12.csv  2011-10-24.csv\n",
      "2011-01-05.csv  2011-03-17.csv  2011-06-03.csv  2011-08-14.csv  2011-10-25.csv\n",
      "2011-01-06.csv  2011-03-18.csv  2011-06-05.csv  2011-08-15.csv  2011-10-26.csv\n",
      "2011-01-07.csv  2011-03-20.csv  2011-06-06.csv  2011-08-16.csv  2011-10-27.csv\n",
      "2011-01-09.csv  2011-03-21.csv  2011-06-07.csv  2011-08-17.csv  2011-10-28.csv\n",
      "2011-01-10.csv  2011-03-22.csv  2011-06-08.csv  2011-08-18.csv  2011-10-30.csv\n",
      "2011-01-11.csv  2011-03-23.csv  2011-06-09.csv  2011-08-19.csv  2011-10-31.csv\n",
      "2011-01-12.csv  2011-03-24.csv  2011-06-10.csv  2011-08-21.csv  2011-11-01.csv\n",
      "2011-01-13.csv  2011-03-25.csv  2011-06-12.csv  2011-08-22.csv  2011-11-02.csv\n",
      "2011-01-14.csv  2011-03-27.csv  2011-06-13.csv  2011-08-23.csv  2011-11-03.csv\n",
      "2011-01-16.csv  2011-03-28.csv  2011-06-14.csv  2011-08-24.csv  2011-11-04.csv\n",
      "2011-01-17.csv  2011-03-29.csv  2011-06-15.csv  2011-08-25.csv  2011-11-06.csv\n",
      "2011-01-18.csv  2011-03-30.csv  2011-06-16.csv  2011-08-26.csv  2011-11-07.csv\n",
      "2011-01-19.csv  2011-03-31.csv  2011-06-17.csv  2011-08-28.csv  2011-11-08.csv\n",
      "2011-01-20.csv  2011-04-01.csv  2011-06-19.csv  2011-08-30.csv  2011-11-09.csv\n",
      "2011-01-21.csv  2011-04-03.csv  2011-06-20.csv  2011-08-31.csv  2011-11-10.csv\n",
      "2011-01-23.csv  2011-04-04.csv  2011-06-21.csv  2011-09-01.csv  2011-11-11.csv\n",
      "2011-01-24.csv  2011-04-05.csv  2011-06-22.csv  2011-09-02.csv  2011-11-13.csv\n",
      "2011-01-25.csv  2011-04-06.csv  2011-06-23.csv  2011-09-04.csv  2011-11-14.csv\n",
      "2011-01-26.csv  2011-04-07.csv  2011-06-24.csv  2011-09-05.csv  2011-11-15.csv\n",
      "2011-01-27.csv  2011-04-08.csv  2011-06-26.csv  2011-09-06.csv  2011-11-16.csv\n",
      "2011-01-28.csv  2011-04-10.csv  2011-06-27.csv  2011-09-07.csv  2011-11-17.csv\n",
      "2011-01-30.csv  2011-04-11.csv  2011-06-28.csv  2011-09-08.csv  2011-11-18.csv\n",
      "2011-01-31.csv  2011-04-12.csv  2011-06-29.csv  2011-09-09.csv  2011-11-20.csv\n",
      "2011-02-01.csv  2011-04-13.csv  2011-06-30.csv  2011-09-11.csv  2011-11-21.csv\n",
      "2011-02-02.csv  2011-04-14.csv  2011-07-01.csv  2011-09-12.csv  2011-11-22.csv\n",
      "2011-02-03.csv  2011-04-15.csv  2011-07-03.csv  2011-09-13.csv  2011-11-23.csv\n",
      "2011-02-04.csv  2011-04-17.csv  2011-07-04.csv  2011-09-14.csv  2011-11-24.csv\n",
      "2011-02-06.csv  2011-04-18.csv  2011-07-05.csv  2011-09-15.csv  2011-11-25.csv\n",
      "2011-02-07.csv  2011-04-19.csv  2011-07-06.csv  2011-09-16.csv  2011-11-27.csv\n",
      "2011-02-08.csv  2011-04-20.csv  2011-07-07.csv  2011-09-18.csv  2011-11-28.csv\n",
      "2011-02-09.csv  2011-04-21.csv  2011-07-08.csv  2011-09-19.csv  2011-11-29.csv\n",
      "2011-02-10.csv  2011-04-26.csv  2011-07-10.csv  2011-09-20.csv  2011-11-30.csv\n",
      "2011-02-11.csv  2011-04-27.csv  2011-07-11.csv  2011-09-21.csv  2011-12-01.csv\n",
      "2011-02-13.csv  2011-04-28.csv  2011-07-12.csv  2011-09-22.csv  2011-12-02.csv\n",
      "2011-02-14.csv  2011-05-01.csv  2011-07-13.csv  2011-09-23.csv  2011-12-04.csv\n",
      "2011-02-15.csv  2011-05-03.csv  2011-07-14.csv  2011-09-25.csv  2011-12-05.csv\n",
      "2011-02-16.csv  2011-05-04.csv  2011-07-15.csv  2011-09-26.csv  2011-12-06.csv\n",
      "2011-02-17.csv  2011-05-05.csv  2011-07-17.csv  2011-09-27.csv  2011-12-07.csv\n",
      "2011-02-18.csv  2011-05-06.csv  2011-07-18.csv  2011-09-28.csv  2011-12-08.csv\n",
      "2011-02-20.csv  2011-05-08.csv  2011-07-19.csv  2011-09-29.csv  2011-12-09.csv\n"
     ]
    }
   ],
   "source": [
    "ls ./spark/bydata/by-day/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b9ac2773-89f6-48c4-b457-16885d8ddf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "origin_file_path = './spark/bydata/by-day/*.csv'\n",
    "df1 = spark.read.format('csv').options(header='true',inferSchema = 'true').load(origin_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4856cd3d-2a5e-4b92-9102-9551dd915676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d69b21ba-41f2-4f62-b2e0-e9981d10d9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|        Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+-------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084| RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077|DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "+---------+---------+-------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2cdf02f2-9261-4357-bf4f-e33393fa87f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country -- groupby count -->  df2 save\n",
    "# df3 CustomerID  count\n",
    "# df4  InvoiceNo  count\n",
    "def getCountGroupBy(dataframe,colname):\n",
    "    return dataframe.groupBy(colname).count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7697cc57-6fee-4d4e-a4e4-a89ed703bea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2 = getCountGroupBy(df1,'Country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b0eb1132-fe95-40fc-8a49-a1ee196ab20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df3 = getCountGroupBy(df1,'CustomerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "08be0003-f13e-41fb-b1b4-0144fe8a35ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df4 = getCountGroupBy(df1,'InvoiceNo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2ad5442b-1555-4f68-b516-49800e73aa91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b6043b61-da00-45f5-be5f-40297562e8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7213a1cf-ec54-4dca-b487-03816d834b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = getCountGroupBy(df1,'Country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "74cda14b-1d6f-4529-9988-8a85857c8a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = getCountGroupBy(df1,'CustomerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b1b31c55-0757-458f-a626-61172f536880",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = getCountGroupBy(df1,'InvoiceNo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3702d2bc-41de-47a7-9305-ec1fdfb8acea",
   "metadata": {},
   "source": [
    "### 이벤트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c961c8-f285-44a6-87f5-d27753b4cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이벤트\n",
    "spark.conf.set('spark.sql.shuffle.partitions',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42770eb1-cb18-462a-b5e4-a991d0311bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "static = spark.read.json('activity-data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60119ca1-e1fb-4e0b-a83a-24f2c2af90ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = spark.readStream.schema(static.schema).option('maxFilePerTrigger',10).json('activity-data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df352297-9f19-4ad8-ae99-6e4fc229f35e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "| Arrival_Time|      Creation_Time|  Device|Index| Model|User|   gt|           x|           y|           z|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "|1424686735090|1424686733090638193|nexus4_1|   18|nexus4|   g|stand| 3.356934E-4|-5.645752E-4|-0.018814087|\n",
      "|1424686735292|1424688581345918092|nexus4_2|   66|nexus4|   g|stand|-0.005722046| 0.029083252| 0.005569458|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d09bc205-ab28-4665-80d7-65286894356f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000000.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e+9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "607dcf8b-69ba-4fc7-add6-0476f6076005",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventTime = streaming.selectExpr(\"*\", \"cast(cast(Creation_Time as double)/1000000000 as timestamp) as event_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34b5fbbf-cd6c-48b9-86c0-71002d40a67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Arrival_Time: bigint, Creation_Time: bigint, Device: string, Index: bigint, Model: string, User: string, gt: string, x: double, y: double, z: double, event_time: timestamp]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eventTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25eadcff-7bfa-4e2f-b89a-5bc3315a7069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/26 13:36:43 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a782a166-92dd-4cbe-97fd-db782522a37d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/26 13:36:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7ff335379430>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,window\n",
    "eventTime.groupBy(window(col(\"event_time\"),\"10 minutes\")).count().writeStream.queryName(\"pyevents_per_window\").format(\"memory\")\\\n",
    ".outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c78e688a-f233-41ba-ba3d-5a82c1243e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/26 13:38:51 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "new_spark_session = SparkSession.builder.appName(\"new_spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5803a199-35a7-4a1f-8798-d977a5012043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|              window| count|\n",
      "+--------------------+------+\n",
      "|{2015-02-23 19:40...| 88681|\n",
      "|{2015-02-24 20:50...|150773|\n",
      "|{2015-02-24 22:00...|133323|\n",
      "+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "+--------------------+------+\n",
      "|              window| count|\n",
      "+--------------------+------+\n",
      "|{2015-02-23 19:40...| 88681|\n",
      "|{2015-02-24 20:50...|150773|\n",
      "|{2015-02-24 22:00...|133323|\n",
      "+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "+--------------------+------+\n",
      "|              window| count|\n",
      "+--------------------+------+\n",
      "|{2015-02-23 19:40...| 88681|\n",
      "|{2015-02-24 20:50...|150773|\n",
      "|{2015-02-24 22:00...|133323|\n",
      "+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "+--------------------+------+\n",
      "|              window| count|\n",
      "+--------------------+------+\n",
      "|{2015-02-23 19:40...| 88681|\n",
      "|{2015-02-24 20:50...|150773|\n",
      "|{2015-02-24 22:00...|133323|\n",
      "+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "+--------------------+------+\n",
      "|              window| count|\n",
      "+--------------------+------+\n",
      "|{2015-02-23 19:40...| 88681|\n",
      "|{2015-02-24 20:50...|150773|\n",
      "|{2015-02-24 22:00...|133323|\n",
      "+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "+--------------------+------+\n",
      "|              window| count|\n",
      "+--------------------+------+\n",
      "|{2015-02-23 19:40...| 88681|\n",
      "|{2015-02-24 20:50...|150773|\n",
      "|{2015-02-24 22:00...|133323|\n",
      "+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "+--------------------+------+\n",
      "|              window| count|\n",
      "+--------------------+------+\n",
      "|{2015-02-23 19:40...| 88681|\n",
      "|{2015-02-24 20:50...|150773|\n",
      "|{2015-02-24 22:00...|133323|\n",
      "+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "+--------------------+------+\n",
      "|              window| count|\n",
      "+--------------------+------+\n",
      "|{2015-02-23 19:40...| 88681|\n",
      "|{2015-02-24 20:50...|150773|\n",
      "|{2015-02-24 22:00...|133323|\n",
      "+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "+--------------------+------+\n",
      "|              window| count|\n",
      "+--------------------+------+\n",
      "|{2015-02-23 19:40...| 88681|\n",
      "|{2015-02-24 20:50...|150773|\n",
      "|{2015-02-24 22:00...|133323|\n",
      "+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "+--------------------+------+\n",
      "|              window| count|\n",
      "+--------------------+------+\n",
      "|{2015-02-23 19:40...| 88681|\n",
      "|{2015-02-24 20:50...|150773|\n",
      "|{2015-02-24 22:00...|133323|\n",
      "+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "for i in range(10):\n",
    "    result = new_spark_session.sql(\"select * from pyevents_per_window\").show(3)\n",
    "    print(result)\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d3502d-94ca-4b2f-9ff2-1736ccb49a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c8f47-bf50-4d8e-86a6-16d27554a25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b522547e-9cd9-442e-9330-8692dba26f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc2777-eb5e-4896-b0d7-d9b17097a29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d2fc7-be4f-4756-b9a5-7eee166bd984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4094d908-14c7-40d1-b162-d3cb70eb5efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/26 13:23:48 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-86996183-9c6d-4faf-a9cb-6951b0a1b5bd. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/26 13:23:48 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f21b8072280>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "eventTime.groupBy(window(col(\"event_time\"),\"10 minutes\",\"5 minutes\")).count().writeStream.queryName(\"pyevents_per_window2\").format(\"memory\")\\\n",
    ".outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dc884ff6-c5dd-4980-9f6c-1a66fa21d596",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43meventTime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithWatermark\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevent_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m30 minutes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\\\n\u001b[1;32m      2\u001b[0m \u001b[38;5;241m.\u001b[39mgroupBy(window(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_time\u001b[39m\u001b[38;5;124m\"\u001b[39m),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10 minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5 minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;241m.\u001b[39mwriteStream\u001b[38;5;241m.\u001b[39mqueryName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyevents_per_window3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      3\u001b[0m \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/dataframe.py:1147\u001b[0m, in \u001b[0;36mDataFrame.withWatermark\u001b[0;34m(self, eventTime, delayThreshold)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m delayThreshold \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(delayThreshold) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   1140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1141\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1142\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1145\u001b[0m         },\n\u001b[1;32m   1146\u001b[0m     )\n\u001b[0;32m-> 1147\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithWatermark\u001b[49m\u001b[43m(\u001b[49m\u001b[43meventTime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelayThreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "eventTime.withWatermark(\"event_time\",\"30 minutes\")\\\n",
    ".groupBy(window(col(\"event_time\"),\"10 minutes\",\"5 minutes\")).count().writeStream.queryName(\"pyevents_per_window3\").format(\"memory\")\\\n",
    ".outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "730a263b-82fb-4ccf-a548-4a037c3ab41e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43meventTime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithWatermark\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevent_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m5 seconds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\\\n\u001b[1;32m      2\u001b[0m \u001b[38;5;241m.\u001b[39mdropDuplicates([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_time\u001b[39m\u001b[38;5;124m\"\u001b[39m])\\\n\u001b[1;32m      3\u001b[0m \u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;241m.\u001b[39mwriteStream\u001b[38;5;241m.\u001b[39mqueryName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyevents_per_window4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      4\u001b[0m \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/dataframe.py:1147\u001b[0m, in \u001b[0;36mDataFrame.withWatermark\u001b[0;34m(self, eventTime, delayThreshold)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m delayThreshold \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(delayThreshold) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   1140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1141\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1142\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1145\u001b[0m         },\n\u001b[1;32m   1146\u001b[0m     )\n\u001b[0;32m-> 1147\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithWatermark\u001b[49m\u001b[43m(\u001b[49m\u001b[43meventTime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelayThreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "eventTime.withWatermark(\"event_time\",\"5 seconds\")\\\n",
    ".dropDuplicates([\"User\",\"event_time\"])\\\n",
    ".groupBy(\"User\").count().writeStream.queryName(\"pyevents_per_window4\").format(\"memory\")\\\n",
    ".outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d150ff01-e3ea-411c-8769-d58bd71f7e70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
