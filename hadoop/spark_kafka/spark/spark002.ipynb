{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee8144a-fab3-44b7-8f32-15335ac9525f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/20 15:19:25 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark =  SparkSession.builder.appName(\"demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ba009-b84c-49b5-9615-666fb6171568",
   "metadata": {},
   "source": [
    "## Create a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25430953-886c-4800-bc15-fcca2bc975d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (\"lee\",32),(\"kim\",20),(\"hong\",30),(\"cho\",50)\n",
    "]\n",
    ", [\"name\",\"age\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e0b5393-5cb9-43bb-b0ba-33bd62e23958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| lee| 32|\n",
      "| kim| 20|\n",
      "|hong| 30|\n",
      "| cho| 50|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9efeeca7-5d7a-486e-9bd5-9f5797c66b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e6a9018-3793-4974-9f09-e6119b9f5414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column life_stage\n",
    "df1 = df.withColumn(\n",
    "    'life_state',\n",
    "    when(col('age')<30,'young')\n",
    "    .when(col('age').between(20,30),'middle')\n",
    "    .otherwise(\"old\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4861a341-f416-41ba-8a72-21541686adbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+\n",
      "|name|age|life_state|\n",
      "+----+---+----------+\n",
      "| lee| 32|       old|\n",
      "| kim| 20|     young|\n",
      "|hong| 30|    middle|\n",
      "| cho| 50|       old|\n",
      "+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3086043-e663-4097-b068-ee3eaf16ff8b",
   "metadata": {},
   "source": [
    "## filterring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38234a73-2f67-42ba-830e-86f76c628cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+\n",
      "|name|age|life_state|\n",
      "+----+---+----------+\n",
      "| lee| 32|       old|\n",
      "| cho| 50|       old|\n",
      "+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# life_sate --> old \n",
    "# where( ).isin([])\n",
    "df1.where(col('life_state').isin(['old'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "480eaea9-66e9-46f4-916a-f75906aeb423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 데이터프레임의  age컬럼의  avg 평균을 구해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e4b4b37-1236-4981-bd23-299f1bc0eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"view_df1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a414b0a2-1a05-4b6e-bf4d-46ecb279e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_df1 = spark.sql(\"\"\"\n",
    "select avg(age) from view_df1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7d303e0-621a-4fa4-a3e2-fdd1b2c9fa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(age)|\n",
      "+--------+\n",
      "|    33.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65a9faa9-e302-4827-bf67-672a3576a000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(age)|\n",
      "+--------+\n",
      "|    33.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "df1.select(avg('age')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7615e40a-91c9-4540-8673-e76796b4a0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|life_state|avg(age)|\n",
      "+----------+--------+\n",
      "|       old|    41.0|\n",
      "|     young|    20.0|\n",
      "|    middle|    30.0|\n",
      "+----------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1.groupBy('life_state').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a6abc99-d24e-4dfa-9aad-f29fa247d79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(age)|\n",
      "+--------+\n",
      "|    33.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select avg(age) from {df1}\",df1=df1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddefc2ba-d766-4253-9a68-85098afc1433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|life_state|avg(age)|\n",
      "+----------+--------+\n",
      "|       old|    41.0|\n",
      "|     young|    20.0|\n",
      "|    middle|    30.0|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select life_state, avg(age) from {df1} group by life_state\", df1=df1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849f28f2-13c1-4723-bf93-a2788cf50844",
   "metadata": {},
   "source": [
    "## SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "539050e7-ea05-419b-8bd7-525bc8ce817d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/20 16:08:26 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/03/20 16:08:26 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/03/20 16:08:32 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "24/03/20 16:08:32 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore root@127.0.0.1\n",
      "24/03/20 16:08:32 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "24/03/20 16:08:35 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/03/20 16:08:35 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/03/20 16:08:35 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/03/20 16:08:35 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/03/20 16:08:36 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    }
   ],
   "source": [
    "df1.write.saveAsTable(\"some_people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ee3eb9f-6316-4649-8c7c-cb9eab73e2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "| name|age|life_state|\n",
      "+-----+---+----------+\n",
      "| hong| 30|    middle|\n",
      "|  kim| 20|     young|\n",
      "|frank| 55|       old|\n",
      "|  cho| 50|       old|\n",
      "|  lee| 32|       old|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from some_people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf9f5345-f14b-47e3-9c53-f58d53a11e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"insert into some_people values('frank',55,'old')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75a16e79-1c1c-4b82-b142-942fc0237e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "| name|age|life_state|\n",
      "+-----+---+----------+\n",
      "|frank| 55|       old|\n",
      "|  cho| 50|       old|\n",
      "|  lee| 32|       old|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from some_people where life_state ='old'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70501d40-6d43-4fe8-a7af-5064d81d5343",
   "metadata": {},
   "source": [
    "## Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6441dab5-5f0b-4b29-80c3-e60c3f19ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet : Aparch Parquet 형식의 테이블, 대규모데이터 세트를 저장하고 처리하는데 사용되는 오픈소스 파일형식\n",
    "# kafka에서 데이터를 읽어와서 Parquet 테이블에 시간별로 쓰는 방법...\n",
    "# kafka 스트림은 지속적으로 데이터가 채워지는 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2dc3cc0-a562-4e8a-8c35-83605527da93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'student_name': 'lee', 'graduation_year': '2022', 'major': 'korean'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"student_name\":\"hong\",\"graduation_year\":\"2023\",\"major\":\"math\"}\n",
    "{\"student_name\":\"lee\",\"graduation_year\":\"2022\",\"major\":\"korean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9403601e-ca88-43db-9584-f8f0bb68d3e9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subscribeTopic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m     spark\u001b[38;5;241m.\u001b[39mreadStream\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkafka\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkafka.bootstrap.servers\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhost1:port1,host2:port2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msSubscribe\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[43msubscribeTopic\u001b[49m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subscribeTopic' is not defined"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    spark.readStream.format('kafka')\n",
    "    .option('kafka.bootstrap.servers','host1:port1,host2:port2')\n",
    "    .option('subscribe',subscribeTopic)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a9b1df-19a2-453f-8283-44ae53c5ee9a",
   "metadata": {},
   "source": [
    "## 구조적 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36d3b4de-48fc-45c2-a1a0-c95b6bc9173f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(number + 10)|\n",
      "+-------------+\n",
      "|           10|\n",
      "|           11|\n",
      "|           12|\n",
      "|           13|\n",
      "|           14|\n",
      "|           15|\n",
      "|           16|\n",
      "|           17|\n",
      "|           18|\n",
      "|           19|\n",
      "|           20|\n",
      "|           21|\n",
      "|           22|\n",
      "|           23|\n",
      "|           24|\n",
      "|           25|\n",
      "|           26|\n",
      "|           27|\n",
      "|           28|\n",
      "|           29|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(500).toDF(\"number\")\n",
    "df.select(df['number'] + 10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25a971c8-21d0-4e0f-a025-23c8dc0c92e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71598607-9860-46ac-bfef-c4911958b7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bafed23f-3b3a-49fe-a988-404d13d4b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField,StructType,StringType,LongType\n",
    "myManualSchema = StructType([\n",
    "    StructField('DEST_COUNTRY_NAME', StringType(), True), \n",
    "    StructField('ORIGIN_COUNTRY_NAME', StringType(), True), \n",
    "    StructField('count', LongType(), True)\n",
    "])\n",
    "df = spark.read.format(\"json\").schema(myManualSchema).load(\"2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a33fecd0-a158-4852-9280-f81b709576c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "|    United States|\n",
      "|            Egypt|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,column\n",
    "# col(\"column name)\n",
    "df.select(column(\"DEST_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "027b2ee1-13e0-462e-8167-a821e6d5ec37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1| 10|\n",
      "|  2| 20|\n",
      "|  3| 30|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 문자열 표현식을 사용해서 컬럼간 논리 연산을 수행 expr()\n",
    "example_df = spark.createDataFrame(\n",
    "    [(1,10),(2,20),(3,30)],['A','B']\n",
    ")\n",
    "example_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "648d068d-2ee3-4e0e-87d0-d1fe43eac522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1| 10|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "example_df.filter( expr('   (((A+1)*5)+2) = B'  )).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "16128964-eb89-4e49-b44a-92f462d1d2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1| 10|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_df.filter( expr('A == 1')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5f551c-a77d-4532-ae92-7fb2edaebb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df.filter( expr('A = 1')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "54c2915a-e411-4ad5-aa25-ee90b6fa95d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row class 는  새로운 행을 만들어 준다\n",
    "from pyspark.sql import Row\n",
    "myRow = Row(\"hello\",'None','1','False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b23a8bc7-3037-4067-9795-44079b7f4521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "68b8b8b9-273d-4e60-b6c4-f47c3b5f8400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+-----+\n",
      "|    a|   b|  c|    d|\n",
      "+-----+----+---+-----+\n",
      "|hello|None|  1|False|\n",
      "+-----+----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame([myRow],['a','b','c','d']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "79fe668a-e43f-48fa-a5cd-98f172af8aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+-----+\n",
      "|    a|   b|  c|    d|\n",
      "+-----+----+---+-----+\n",
      "|hello|NULL|  1|false|\n",
      "+-----+----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField,StructType,StringType,LongType,IntegerType,BooleanType\n",
    "myRow = Row(\"hello\",None,1,False)\n",
    "myManualSchema = StructType([\n",
    "    StructField('a', StringType(), True), \n",
    "    StructField('b', IntegerType(),True),\n",
    "    StructField('c', IntegerType(), True),\n",
    "    StructField('d', BooleanType(), True)\n",
    "])\n",
    "spark.createDataFrame([myRow],myManualSchema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "572d6541-94c6-4e77-a691-73baa8bb7ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('DEST_COUNTRY_NAME').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ca9f7b71-c7f3-478a-80d5-6b11d2624810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|DEST_COUNTRY_NAME|count|\n",
      "+-----------------+-----+\n",
      "|    United States|   15|\n",
      "|    United States|    1|\n",
      "+-----------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('DEST_COUNTRY_NAME','count').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1ef4406c-dca1-45c0-8a7b-f2aafa26ab3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------------+-----------------+\n",
      "|          aaa|          bbb|          ccc|DEST_COUNTRY_NAME|\n",
      "+-------------+-------------+-------------+-----------------+\n",
      "|United States|United States|United States|    United States|\n",
      "|United States|United States|United States|    United States|\n",
      "+-------------+-------------+-------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    expr('DEST_COUNTRY_NAME as aaa'),\n",
    "    col('DEST_COUNTRY_NAME').alias('bbb'),\n",
    "    column('DEST_COUNTRY_NAME').alias('ccc'),\n",
    "    'DEST_COUNTRY_NAME'\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3e597783-2403-44cd-975d-dd0aebdc94d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"avg(count)\",\"count(distinct(DEST_COUNTRY_NAME))\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4967856-f251-4608-8273-987c6df6b0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
